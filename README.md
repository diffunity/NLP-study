# Natural Language Processing (NLP)
Natural Language Processing (NLP) related studies and practice
##  Papers

#### Word embeddings

- [Efficient Representation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [Ditributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
- [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf)

- [FastText](https://arxiv.org/pdf/1607.04606.pdf)

- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) 

  

#### seq2seq

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)

* [Text Understanding with the Attention Sum Reader Network](https://arxiv.org/abs/1603.01547) 

* ELMo | [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365)

* Transformer | [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

* T5 | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)

* CTRL | [A Conditional Transformer Language Model For Controllable Generation](https://arxiv.org/pdf/1909.05858.pdf)

* OpenAI GPT | [Improving Language Understanding by Generative Pre-Training](https://pdfs.semanticscholar.org/cd18/800a0fe0b668a1cc19f2ec95b5003d0a5035.pdf?_ga=2.40131109.647956165.1588658157-925505706.1588658157)

* BERT | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

* RoBERTa | [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

* ALBERT | [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)

* XLNet | [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

  

#### Others

- [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364)

* [Adversarial Multi-task Learning for Text Classification](https://arxiv.org/abs/1704.05742) 

  

##  Libraries
* Pytorch (1.5.x)
* Tensorflow (2.x.x)
* Hugging Face
* nltk
